{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install youtokentome rouge razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import youtokentome as yttm\n",
    "import razdel\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from rouge import Rouge\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = cleaned_dataset[:100000]\n",
    "val_records = cleaned_dataset[100000:115000]\n",
    "test_records = cleaned_dataset[115000:130000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils: score calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_scores(references, predictions, metric=\"all\"):\n",
    "    print(\"Count:\", len(predictions))\n",
    "    print(\"Last true headline:\", references[-1])\n",
    "    print(\"Last predicted headline:\", predictions[-1])\n",
    "\n",
    "    if metric in (\"bleu\", \"all\"):\n",
    "        print(\"\\nBLEU: \", corpus_bleu([[r] for r in references], predictions))\n",
    "    if metric in (\"rouge\", \"all\"):\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(predictions, references, avg=True)\n",
    "        scores_string = \"\"\n",
    "        for metric, value in scores.items():\n",
    "            scores_string += \"\\n\" + str(metric) + \":\" + str(value)\n",
    "        print(\"ROUGE: \", scores_string, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(records, model_path, model_type=\"bpe\", vocab_size=10000, lower=True):\n",
    "    temp_file_name = \"temp.txt\"\n",
    "    with open(temp_file_name, \"w\") as temp:\n",
    "        for text, title in tqdm(records[['Text', 'Head_title']].values):\n",
    "            if lower:\n",
    "                title = title.lower()\n",
    "                text = text.lower()\n",
    "            if not text or not title:\n",
    "                continue\n",
    "            temp.write(text + \"\\n\")\n",
    "            temp.write(title + \"\\n\")\n",
    "    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bpe(train_records, \"BPE_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_processor = yttm.BPE('BPE_model.bin')\n",
    "bpe_processor.encode([\"шустрая бурая лиса прыгает через ленивого пса\"], output_type=yttm.OutputType.SUBWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составим словарь для индексации токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, bpe_processor):\n",
    "        self.index2word = bpe_processor.vocab()\n",
    "        self.word2index = {w: i for i, w in enumerate(self.index2word)}\n",
    "        self.word2count = Counter()\n",
    "\n",
    "    def get_pad(self):\n",
    "        return self.word2index[\"<PAD>\"]\n",
    "\n",
    "    def get_sos(self):\n",
    "        return self.word2index[\"<SOS>\"]\n",
    "\n",
    "    def get_eos(self):\n",
    "        return self.word2index[\"<EOS>\"]\n",
    "\n",
    "    def get_unk(self):\n",
    "        return self.word2index[\"<UNK>\"]\n",
    "    \n",
    "    def has_word(self, word) -> bool:\n",
    "        return word in self.word2index\n",
    "\n",
    "    def get_index(self, word):\n",
    "        if word in self.word2index:\n",
    "            return self.word2index[word]\n",
    "        return self.get_unk()\n",
    "\n",
    "    def get_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.index2word)\n",
    "\n",
    "    def is_empty(self):\n",
    "        empty_size = 4\n",
    "        return self.size() <= empty_size\n",
    "\n",
    "    def reset(self):\n",
    "        self.word2count = Counter()\n",
    "        self.index2word = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
    "        self.word2index = {word: index for index, word in enumerate(self.index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Vocabulary(bpe_processor)\n",
    "vocabulary.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кэш oracle summary.\n",
    "Закэшируем oracle summary, чтобы не пересчитывать их каждый раз."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n",
    "    rouge = Rouge()\n",
    "    sentences_ = []\n",
    "    oracle_sentences_ = []\n",
    "    oracle_summary_ = []\n",
    "    records = records.iloc[:nrows].copy()\n",
    "\n",
    "    for text, title in tqdm(records[['Text', 'Head_title']].values):\n",
    "        title = title.lower() if lower else title\n",
    "        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, title, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n",
    "                                                                         lower=lower, max_sentences=max_sentences)\n",
    "        sentences_ += [sentences]\n",
    "        oracle_sentences_ += [list(sentences_indicies)]\n",
    "        oracle_summary_ += [oracle_summary]\n",
    "    records['sentences'] = sentences_\n",
    "    records['oracle_sentences'] = oracle_sentences_\n",
    "    records['oracle_summary'] = oracle_summary_\n",
    "    return records\n",
    "\n",
    "\n",
    "def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n",
    "    '''\n",
    "    Жадное построение oracle summary\n",
    "    '''\n",
    "    gold_summary = gold_summary.lower() if lower else gold_summary\n",
    "    # Делим текст на предложения\n",
    "    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n",
    "    n_sentences = len(sentences)\n",
    "    oracle_summary_sentences = set()\n",
    "    score = -1.0\n",
    "    summaries = []\n",
    "    for _ in range(min(n_sentences, 2)):\n",
    "        for i in range(n_sentences):\n",
    "            if i in oracle_summary_sentences:\n",
    "                continue\n",
    "            current_summary_sentences = copy.copy(oracle_summary_sentences)\n",
    "            # Добавляем какое-то предложения к уже существующему summary\n",
    "            current_summary_sentences.add(i)\n",
    "            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n",
    "            # Считаем метрики\n",
    "            current_score = calc_score(current_summary, gold_summary)\n",
    "            summaries.append((current_score, current_summary_sentences))\n",
    "        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n",
    "        # Иначе на этом заканчиваем\n",
    "        best_summary_score, best_summary_sentences = max(summaries)\n",
    "        if best_summary_score <= score:\n",
    "            break\n",
    "        oracle_summary_sentences = best_summary_sentences\n",
    "        score = best_summary_score\n",
    "    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n",
    "    return oracle_summary, oracle_summary_sentences\n",
    "\n",
    "\n",
    "def calc_single_score(pred_summary, gold_summary, rouge):\n",
    "    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_train_records = add_oracle_summary_to_records(train_records, nrows=10000)\n",
    "ext_val_records = add_oracle_summary_to_records(val_records, nrows=4000)\n",
    "ext_test_records = add_oracle_summary_to_records(test_records, nrows=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составление батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtDataset(data.Dataset):\n",
    "    def __init__(self, records, vocabulary, bpe_processor, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n",
    "        self.records = records\n",
    "        self.num_samples = records.shape[0]\n",
    "        self.bpe_processor = bpe_processor\n",
    "        self.lower = lower\n",
    "        self.rouge = Rouge()\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_sentence_length = max_sentence_length\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.records.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cur_record = self.records.iloc[idx]\n",
    "        inputs = list(map(lambda x: x[:self.max_sentence_length], self.bpe_processor.encode(cur_record['sentences'], output_type=yttm.OutputType.ID)))\n",
    "        outputs = [int(i in cur_record['oracle_sentences']) for i in range(len(cur_record['sentences']))]\n",
    "        return {'inputs': inputs, 'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ExtDataset(ext_train_records, vocabulary, bpe_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(records):\n",
    "    max_length = max(len(sentence) for record in records for sentence in record['inputs'])\n",
    "    max_sentences = max(len(record['outputs']) for record in records)\n",
    "\n",
    "    new_inputs = torch.zeros((len(records), max_sentences, max_length))\n",
    "    new_outputs = torch.zeros((len(records), max_sentences))\n",
    "    for i, record in enumerate(records):\n",
    "        for j, sentence in enumerate(record['inputs']):\n",
    "            new_inputs[i, j, :len(sentence)] += np.array(sentence)\n",
    "        new_outputs[i, :len(record['outputs'])] += np.array(record['outputs'])\n",
    "    return {'features': new_inputs.type(torch.LongTensor), 'targets': new_outputs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3, bidirectional=True):\n",
    "        super(SentenceEncoderRNN, self).__init__()\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        assert hidden_size % num_directions == 0\n",
    "        hidden_size = hidden_size // num_directions\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n",
    "        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        embedded = self.embedding_layer(inputs)\n",
    "        outputs, _ = self.rnn_layer(embedded, hidden)\n",
    "        sentences_embeddings = torch.mean(outputs, 1)\n",
    "        return sentences_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTaggerRNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 token_embedding_dim=256,\n",
    "                 sentence_encoder_hidden_size=256,\n",
    "                 hidden_size=256,\n",
    "                 bidirectional=True,\n",
    "                 sentence_encoder_n_layers=2,\n",
    "                 sentence_encoder_dropout=0.3,\n",
    "                 sentence_encoder_bidirectional=True,\n",
    "                 n_layers=1,\n",
    "                 dropout=0.3):\n",
    "        super(SentenceTaggerRNN, self).__init__()\n",
    "\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        assert hidden_size % num_directions == 0\n",
    "        hidden_size = hidden_size // num_directions\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.sentence_encoder = SentenceEncoderRNN(vocabulary_size, token_embedding_dim,\n",
    "                                                   sentence_encoder_hidden_size, sentence_encoder_n_layers, \n",
    "                                                   sentence_encoder_dropout, sentence_encoder_bidirectional)\n",
    "        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n",
    "                           bidirectional=bidirectional, batch_first=True)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n",
    "        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs, hidden=None):\n",
    "        batch_size = inputs.size(0)\n",
    "        sentences_count = inputs.size(1)\n",
    "        tokens_count = inputs.size(2)\n",
    "        inputs = inputs.reshape(-1, tokens_count)\n",
    "        embedded_sentences = self.sentence_encoder(inputs)\n",
    "        embedded_sentences = embedded_sentences.reshape(batch_size, sentences_count, -1)\n",
    "        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n",
    "        outputs = self.dropout_layer(outputs)\n",
    "        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n",
    "        content = self.content_linear_layer(outputs).squeeze(2)\n",
    "        salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2)\n",
    "        return content + salience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()  # train mode\n",
    "    avg_loss = 0\n",
    "    for item in tqdm(train_loader):\n",
    "        # data to device\n",
    "        inputs = item['features'].to(DEVICE)\n",
    "        targets = item['targets'].to(DEVICE)\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward\n",
    "        outputs = model(inputs)\n",
    "        # calc batch loss\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # add batch average loss\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()  # testing mode\n",
    "    avg_loss = 0\n",
    "    for item in val_loader:\n",
    "        inputs = item['features'].to(DEVICE)\n",
    "        targets = item['targets'].to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            # calc batch loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            # add batch average loss\n",
    "            avg_loss += loss.item() / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def train(model, opt, criterion, epochs, train_loader, val_loader, verbose=True):\n",
    "    history = []\n",
    "    for epoch in range(epochs):\n",
    "        if verbose:\n",
    "            print('* Epoch %d/%d' % (epoch+1, epochs))\n",
    "        # Обучаем модель, собираем loss-метрику по текущей эпохе\n",
    "        avg_train_loss = fit_epoch(model, train_loader, criterion, opt)\n",
    "        # Собираем loss-метрику на валидационном датасете по текущей эпохе\n",
    "        avg_val_loss = eval_epoch(model, val_loader, criterion)\n",
    "        # Сохраняем все метрики для последующей отрисовки на графиках\n",
    "        history.append((avg_train_loss, avg_val_loss))\n",
    "                \n",
    "        if verbose:\n",
    "            print('Train loss: %f' % avg_train_loss)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(ExtDataset(ext_train_records, vocabulary, bpe_processor=bpe_processor), batch_size=200, collate_fn=collate_fn)\n",
    "val_loader = data.DataLoader(ExtDataset(ext_val_records, vocabulary, bpe_processor=bpe_processor), batch_size=200, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "torch.cuda.manual_seed(13)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "model = SentenceTaggerRNN(vocabulary.size()).to(DEVICE)\n",
    "\n",
    "history = train(model = model,\n",
    "                opt = torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "                criterion = nn.BCEWithLogitsLoss(),\n",
    "                epochs = 5,\n",
    "                train_loader = train_loader,\n",
    "                val_loader = val_loader,\n",
    "                verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "for i, item in tqdm(enumerate(data.DataLoader(ExtDataset(ext_test_records, vocabulary, bpe_processor=bpe_processor), batch_size=1, collate_fn=collate_fn)), total=ext_test_records.shape[0]):\n",
    "    logits = model(item[\"features\"].to(device))[0] # forward\n",
    "    record = ext_test_records.iloc[i]\n",
    "    predicted_summary = []\n",
    "    for i, logit in enumerate(logits):\n",
    "        if logit > 0.0:\n",
    "            predicted_summary.append(record['sentences'][i])\n",
    "    if not predicted_summary:\n",
    "        predicted_summary.append(record['sentences'][torch.max(logits, dim=0)[1].item()])\n",
    "    predicted_summary = \" \".join(predicted_summary)\n",
    "    references.append(record['summary'].lower())\n",
    "    predictions.append(predicted_summary)\n",
    "\n",
    "calc_scores(references, predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}